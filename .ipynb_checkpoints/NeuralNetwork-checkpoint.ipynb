{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent and Backpropagation\n",
    "### Understanding the math behind gradient descent and how to implement backprogagation in python\n",
    "--------------------\n",
    "**Author: Jay Mody**\n",
    "\n",
    "**Required Knowledge:**\n",
    "- Basic Python Skills\n",
    "- Numpy\n",
    "- Calculus (derivatives, gradients, chain rule)\n",
    "- Linear Algebra (matrices, matrix multiplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "#### Imports ####\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# #### Data Parameters ####\n",
    "n_features = 784  # n_input_nodes\n",
    "n_classes = 10 # n_output_nodes\n",
    "\n",
    "colors_list = ['red', 'cyan', 'magenta', 'green', 'black', 'blue']\n",
    "colors = ListedColormap(colors_list)\n",
    "\n",
    "##  Loading Data  ##\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train_raw), (x_test, y_test_raw) = mnist.load_data()\n",
    "\n",
    "# Flattening for mlp\n",
    "x_train = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test = x_test.reshape(x_test.shape[0], 784)\n",
    "\n",
    "print(x_train.shape) #--> (60000, 28, 28)\n",
    "print(y_train_raw.shape) #--> (60000,)\n",
    "print(x_test.shape) #--> (10000, 28, 28)\n",
    "print(y_test_raw.shape) #--> (10000,)\n",
    "\n",
    "##  Normalizing Data  ##\n",
    "normal_val = 255\n",
    "\n",
    "x_train = np.divide(x_train, normal_val)\n",
    "x_test = np.divide(x_test, normal_val)\n",
    "\n",
    "##  Splitting Data into Validation and Train Sets  ##\n",
    "percent_train = 0.80\n",
    "\n",
    "train_samples = int(percent_train * len(x_train))\n",
    "val_x = x_train[train_samples:]\n",
    "x_train = x_train[:train_samples]\n",
    "\n",
    "target_samples = int(percent_train * len(y_train_raw))\n",
    "val_y = y_train_raw[target_samples:]\n",
    "y_train_raw = y_train_raw[:target_samples]\n",
    "\n",
    "## One Hot Encoding Y_train for training ##\n",
    "y_train = np.zeros((y_train_raw.shape[0], n_classes))\n",
    "y_train[np.arange(y_train_raw.size), y_train_raw] = 1\n",
    "\n",
    "## One Hot Encoding Y_test for testing ##\n",
    "y_test = np.zeros((y_test_raw.shape[0], n_classes))\n",
    "y_test[np.arange(y_test_raw.size), y_test_raw] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Imports ####\n",
    "# import sys\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.colors import ListedColormap\n",
    "# from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "# #### Data Parameters ####\n",
    "# n_features = 2  # n_input_nodes\n",
    "# n_classes = 2 # n_output_nodes\n",
    "\n",
    "# n_training_samples = 300 \n",
    "# n_testing_samples = 200\n",
    "\n",
    "# cluster_std = 0.1\n",
    "# center_box = (-1, 1)\n",
    "\n",
    "# colors_list = ['red', 'cyan', 'magenta', 'green', 'black', 'blue']\n",
    "# colors = ListedColormap(colors_list)\n",
    "\n",
    "# seed = 4\n",
    "\n",
    "\n",
    "\n",
    "# #### Training Data ####\n",
    "# x_train, y_train_raw = make_blobs(n_samples = n_training_samples, \n",
    "#                                   n_features = n_features,\n",
    "#                                   centers = n_classes,\n",
    "#                                   center_box = center_box,\n",
    "#                                   cluster_std =  cluster_std, \n",
    "#                                   random_state = seed)\n",
    "\n",
    "# # One-hot encodes the y values (categorically encoding the data)\n",
    "# y_train = np.zeros((y_train_raw.shape[0], n_classes))\n",
    "# y_train[np.arange(y_train_raw.size), y_train_raw] = 1\n",
    "\n",
    "\n",
    "\n",
    "# #### Testing Data ####\n",
    "# x_test, y_test_raw = make_blobs(n_samples = n_testing_samples + n_training_samples, \n",
    "#                             n_features = n_features,\n",
    "#                             centers = n_classes,\n",
    "#                             center_box = center_box,\n",
    "#                             cluster_std =  cluster_std, \n",
    "#                             random_state = seed)\n",
    "# x_test = x_test[-n_testing_samples:]\n",
    "# y_test_raw = y_test_raw[-n_testing_samples:]\n",
    "\n",
    "# # One-hot encodes the y values (categorically encoding the data)\n",
    "# y_test = np.zeros((y_test_raw.shape[0], n_classes))\n",
    "# y_test[np.arange(y_test_raw.size), y_test_raw] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(1, 2, sharey = True, sharex = True, figsize=(16, 6))\n",
    "\n",
    "# _ = ax1.scatter(x_train[:, 0], x_train[:, 1], c = y_train_raw, cmap = colors)\n",
    "# _ = ax1.set_title('training set')\n",
    "\n",
    "# _ = ax2.scatter(x_test[:, 0], x_test[:, 1], c = y_test_raw, cmap = colors)\n",
    "# _ = ax2.set_title('testing set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-13-bbe6498eead8>, line 74)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-bbe6498eead8>\"\u001b[0;36m, line \u001b[0;32m74\u001b[0m\n\u001b[0;31m    d1 = y - y_hat\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#### Neural Network Class ####\n",
    "class NeuralNetwork:\n",
    "    ##### Constructor ####\n",
    "    def __init__(self, n_input_nodes, hidden_nodes, n_output_nodes, lr):\n",
    "        ## Network ##\n",
    "        self.n_input_nodes = n_input_nodes\n",
    "        self.n_output_nodes = n_output_nodes\n",
    "        \n",
    "        self.nodes = hidden_nodes\n",
    "        self.nodes.insert(0, n_input_nodes)\n",
    "        self.nodes.append(n_output_nodes)\n",
    "        \n",
    "        ## Weights and Biases##\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(1, len(self.nodes)):\n",
    "            self.weights.append(np.random.uniform(-1.0, 1.0, (self.nodes[i-1], self.nodes[i])))\n",
    "            self.biases.append(np.random.uniform(-1.0, 1.0, (1, self.nodes[i])))\n",
    "        \n",
    "        ## Learning Rate ##\n",
    "        self.lr = lr\n",
    "        \n",
    "        ## Activation Functions ##\n",
    "        # Linear Activation\n",
    "        self.linear = lambda x: x\n",
    "        self.d_linear = lambda x: np.ones(x.shape)\n",
    "        \n",
    "        # Relu Activation\n",
    "        def relu(x):\n",
    "            x[x<0] = 0\n",
    "            return x\n",
    "        def d_relu(out):\n",
    "            out: x[x>0] = 1\n",
    "            return out\n",
    "        self.relu = relu\n",
    "        self.d_relu = d_relu\n",
    "            \n",
    "        # Sigmoid Activation\n",
    "        self.sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "        self.d_sigmoid = lambda out: out * (1 - out)  # assumes out is tanh(x)\n",
    "        \n",
    "        # Hyperbolic Tangent Activation\n",
    "        self.tanh = lambda x: np.tanh(x)\n",
    "        self.d_tanh = lambda out: 1 - out**2 # assumes out is tanh(x)\n",
    "        \n",
    "    \n",
    "    #### Feed Forward ####\n",
    "    def feed_forward(self, X):\n",
    "        outputs = []\n",
    "        \n",
    "        logits = np.dot(X, self.weights[0]) + self.biases[0]\n",
    "        \n",
    "        for i in range(1, len(self.nodes) - 1):\n",
    "            out = self.relu(logits)\n",
    "            outputs.append(out)\n",
    "            logits = np.dot(out, self.weights[i]) + self.biases[i]\n",
    "        \n",
    "        out = self.sigmoid(logits)\n",
    "        outputs.append(out)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    #### Backpropagation ####\n",
    "    def backpropagation(self, X, y, outputs):\n",
    "        gradients = []\n",
    "        \n",
    "        d1 = y - outputs[-1]\n",
    "        d2 = self.d_sigmoid(outputs[-1])\n",
    "        error = d1 * d2\n",
    "        \n",
    "        error_term = outputs[-2].T * error \n",
    "        gradients.append(error_term)\n",
    "        \n",
    "        for i in range(len(outputs) - 2, 1, -1):\n",
    "            d = self.d_relu(outputs[i])\n",
    "            error = np.dot(error, self.weights[i+1].T) * d\n",
    "            \n",
    "            error_term = outputs[i-1].T * error \n",
    "            gradients.append(error_term)\n",
    "        \n",
    "#         # Output Layer\n",
    "#         d1 = y - y_hat\n",
    "#         d2 = self.d_tanh(y_hat)\n",
    "#         output_error = d1 * d2\n",
    "        \n",
    "#         # Hidden Layer\n",
    "#         d3 = self.weights_h_o\n",
    "#         d4 = self.d_sigmoid(hidden_outputs)\n",
    "#         hidden_error = np.dot(output_error, d3.T) * d4\n",
    "        \n",
    "#         # Gradients\n",
    "#         output_grad = hidden_outputs.T * output_error\n",
    "#         hidden_grad = X.T * hidden_error\n",
    "        \n",
    "        \n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    #### Training ####\n",
    "    def train(self, features, targets):\n",
    "        # Batch Size for weight update step\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        # Delta Weights Variables\n",
    "        delta_weights_i_h = np.zeros(self.weights_i_h.shape)\n",
    "        delta_weights_h_o = np.zeros(self.weights_h_o.shape)\n",
    "        delta_bias_i_h = np.zeros(self.bias_i_h.shape)\n",
    "        delta_bias_h_o = np.zeros(self.bias_h_o.shape)\n",
    "        \n",
    "        # For every data point, forward pass, backpropogation, store weights change\n",
    "        for X, y in zip(features, targets):\n",
    "            # Forward pass\n",
    "            X = X.reshape(1, X.shape[0])\n",
    "            hidden_outputs, y_hat = self.feed_forward(X)\n",
    "            \n",
    "            # Back propogation\n",
    "            hidden_grad, bias_h_grad, output_grad, bias_o_grad = self.backpropagation(X, y, y_hat, hidden_outputs)\n",
    "            \n",
    "            # Weights\n",
    "            delta_weights_i_h += hidden_grad\n",
    "            delta_weights_h_o += output_grad\n",
    "            # Bias\n",
    "            delta_bias_i_h += bias_h_grad\n",
    "            delta_bias_h_o += bias_o_grad\n",
    "        \n",
    "        \n",
    "        # Update Weights and biases\n",
    "        self.weights_i_h += (self.lr * delta_weights_i_h) / batch_size\n",
    "        self.weights_h_o += (self.lr * delta_weights_h_o) / batch_size \n",
    "        self.bias_i_h += (self.lr * delta_bias_i_h) / batch_size\n",
    "        self.bias_h_o += (self.lr * delta_bias_h_o) / batch_size\n",
    "    \n",
    "    #### Testing Methods ####\n",
    "    def predict(self, X):\n",
    "        # Gives prediction\n",
    "        return self.feed_forward(X)[1]\n",
    "    \n",
    "    def test(self, features, targets):\n",
    "        predictions = self.predict(features)\n",
    "\n",
    "        n_correct = 0\n",
    "        for i in range(len(predictions)):\n",
    "            prediction = np.argmax(predictions[i])\n",
    "            correct = np.argmax(targets[i])\n",
    "\n",
    "            if prediction == correct:\n",
    "                n_correct += 1\n",
    "\n",
    "        return n_correct / len(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, y_hat):\n",
    "    return np.mean((y-y_hat)**2)\n",
    "\n",
    "#### Network Parameters ####\n",
    "n_input_nodes = n_features\n",
    "n_output_nodes = n_classes\n",
    "\n",
    "n_hidden_nodes = [32, 16]\n",
    "\n",
    "n_epochs = 500\n",
    "lr = 0.01\n",
    "batch_size = 128\n",
    "\n",
    "#### Neural Network ####\n",
    "network = NeuralNetwork(n_input_nodes, n_hidden_nodes, n_output_nodes, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Train Set: 0.0060625\n",
      "Accuracy on Test Set: 0.0053\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on Train Set:\", network.test(x_train, y_train))\n",
    "print(\"Accuracy on Test Set:\", network.test(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train ####\n",
    "losses = {'train':[], 'validation':[]}\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    batch_indices = np.random.choice(list(range(x_train.shape[0])), size = batch_size)\n",
    "    features = np.array([x_train[i] for i in batch_indices])\n",
    "    targets = np.array([y_train[i] for i in batch_indices])\n",
    "    \n",
    "    network.train(features, targets)\n",
    "    \n",
    "    # Printing out the training progress\n",
    "    train_loss = MSE(network.predict(x_train).T, y_train_raw)\n",
    "    val_loss = MSE(network.predict(x_test).T, y_test_raw)\n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}%\".format(100 * epoch/float(n_epochs)) \\\n",
    "                     + \" ... Training loss: \" + str(train_loss)[:5] \\\n",
    "                     + \" ... Validation loss: \" + str(val_loss)[:5])\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    losses['train'].append(train_loss)\n",
    "    losses['validation'].append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.plot(losses['validation'], label='Validation loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on Train Set:\", network.test(x_train, y_train))\n",
    "print(\"Accuracy on Test Set:\", network.test(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### END OF NOTEBOOK ####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
