{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent and Backpropagation\n",
    "### Understanding the math behind gradient descent and how to implement backprogagation in python\n",
    "--------------------\n",
    "**Author: Jay Mody**\n",
    "\n",
    "**Required Knowledge:**\n",
    "- Basic Python Skills\n",
    "- Numpy\n",
    "- Calculus (derivatives, gradients, chain rule)\n",
    "- Linear Algebra (matrices, matrix multiplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "#### Imports ####\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# #### Data Parameters ####\n",
    "n_features = 784  # n_input_nodes\n",
    "n_classes = 10 # n_output_nodes\n",
    "\n",
    "colors_list = ['red', 'cyan', 'magenta', 'green', 'black', 'blue']\n",
    "colors = ListedColormap(colors_list)\n",
    "\n",
    "##  Loading Data  ##\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train_raw), (x_test, y_test_raw) = mnist.load_data()\n",
    "\n",
    "# Flattening for mlp\n",
    "x_train = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test = x_test.reshape(x_test.shape[0], 784)\n",
    "\n",
    "print(x_train.shape) #--> (60000, 28, 28)\n",
    "print(y_train_raw.shape) #--> (60000,)\n",
    "print(x_test.shape) #--> (10000, 28, 28)\n",
    "print(y_test_raw.shape) #--> (10000,)\n",
    "\n",
    "##  Normalizing Data  ##\n",
    "normal_val = 255\n",
    "\n",
    "x_train = np.divide(x_train, normal_val)\n",
    "x_test = np.divide(x_test, normal_val)\n",
    "\n",
    "##  Splitting Data into Validation and Train Sets  ##\n",
    "percent_train = 0.80\n",
    "\n",
    "train_samples = int(percent_train * len(x_train))\n",
    "val_x = x_train[train_samples:]\n",
    "x_train = x_train[:train_samples]\n",
    "\n",
    "target_samples = int(percent_train * len(y_train_raw))\n",
    "val_y = y_train_raw[target_samples:]\n",
    "y_train_raw = y_train_raw[:target_samples]\n",
    "\n",
    "## One Hot Encoding Y_train for training ##\n",
    "y_train = np.zeros((y_train_raw.shape[0], n_classes))\n",
    "y_train[np.arange(y_train_raw.size), y_train_raw] = 1\n",
    "\n",
    "## One Hot Encoding Y_test for testing ##\n",
    "y_test = np.zeros((y_test_raw.shape[0], n_classes))\n",
    "y_test[np.arange(y_test_raw.size), y_test_raw] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Imports ####\n",
    "# import sys\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.colors import ListedColormap\n",
    "# from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "# #### Data Parameters ####\n",
    "# n_features = 2  # n_input_nodes\n",
    "# n_classes = 3 # n_output_nodes\n",
    "\n",
    "# n_training_samples = 300 \n",
    "# n_testing_samples = 200\n",
    "\n",
    "# cluster_std = 0.5\n",
    "# center_box = (-2, 2)\n",
    "\n",
    "# colors_list = ['red', 'cyan', 'magenta', 'green', 'black', 'blue']\n",
    "# colors = ListedColormap(colors_list)\n",
    "\n",
    "# seed = 45\n",
    "\n",
    "\n",
    "\n",
    "# #### Training Data ####\n",
    "# x_train, y_train_raw = make_blobs(n_samples = n_training_samples, \n",
    "#                                   n_features = n_features,\n",
    "#                                   centers = n_classes,\n",
    "#                                   center_box = center_box,\n",
    "#                                   cluster_std =  cluster_std, \n",
    "#                                   random_state = seed)\n",
    "\n",
    "# # One-hot encodes the y values (categorically encoding the data)\n",
    "# y_train = np.zeros((y_train_raw.shape[0], n_classes))\n",
    "# y_train[np.arange(y_train_raw.size), y_train_raw] = 1\n",
    "\n",
    "\n",
    "\n",
    "# #### Testing Data ####\n",
    "# x_test, y_test_raw = make_blobs(n_samples = n_testing_samples + n_training_samples, \n",
    "#                             n_features = n_features,\n",
    "#                             centers = n_classes,\n",
    "#                             center_box = center_box,\n",
    "#                             cluster_std =  cluster_std, \n",
    "#                             random_state = seed)\n",
    "# x_test = x_test[-n_testing_samples:]\n",
    "# y_test_raw = y_test_raw[-n_testing_samples:]\n",
    "\n",
    "# # One-hot encodes the y values (categorically encoding the data)\n",
    "# y_test = np.zeros((y_test_raw.shape[0], n_classes))\n",
    "# y_test[np.arange(y_test_raw.size), y_test_raw] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(1, 2, sharey = True, sharex = True, figsize=(16, 6))\n",
    "\n",
    "# _ = ax1.scatter(x_train[:, 0], x_train[:, 1], c = y_train_raw, cmap = colors)\n",
    "# _ = ax1.set_title('training set')\n",
    "\n",
    "# _ = ax2.scatter(x_test[:, 0], x_test[:, 1], c = y_test_raw, cmap = colors)\n",
    "# _ = ax2.set_title('testing set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Neural Network Class ####\n",
    "class NeuralNetwork:\n",
    "    ##### Constructor ####\n",
    "    def __init__(self, n_input_nodes, hidden_nodes, n_output_nodes, lr):\n",
    "        ## Network ##\n",
    "        self.n_input_nodes = n_input_nodes\n",
    "        self.n_output_nodes = n_output_nodes\n",
    "        \n",
    "        self.nodes = hidden_nodes\n",
    "        self.nodes.insert(0, n_input_nodes)\n",
    "        self.nodes.append(n_output_nodes)\n",
    "        \n",
    "        ## Weights and Biases##\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(1, len(self.nodes)):\n",
    "            self.weights.append(np.random.uniform(-1.0, 1.0, (self.nodes[i-1], self.nodes[i])))\n",
    "            self.biases.append(np.random.uniform(-1.0, 1.0, (1, self.nodes[i])))\n",
    "        \n",
    "        ## Learning Rate ##\n",
    "        self.lr = lr\n",
    "        \n",
    "        ## Activation Functions ##\n",
    "        # Linear Activation\n",
    "        self.linear = lambda x: x\n",
    "        self.d_linear = lambda x: np.ones(x.shape)\n",
    "        \n",
    "        # Relu Activation\n",
    "        def relu(x):\n",
    "            x[x<0] = 0\n",
    "            return x\n",
    "        def d_relu(out):\n",
    "            out: x[x>0] = 1\n",
    "            return out\n",
    "        self.relu = relu\n",
    "        self.d_relu = d_relu\n",
    "            \n",
    "        # Sigmoid Activation\n",
    "        self.sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "        self.d_sigmoid = lambda out: out * (1 - out)  # assumes out is tanh(x)\n",
    "        \n",
    "        # Hyperbolic Tangent Activation\n",
    "        self.tanh = lambda x: np.tanh(x)\n",
    "        self.d_tanh = lambda out: 1 - out**2 # assumes out is tanh(x)\n",
    "        \n",
    "    def getWeights(self):\n",
    "        return self.weights.copy()\n",
    "    def getBiases(self):\n",
    "        return self.biases.copy()\n",
    "    \n",
    "    def setWeights(self, weights):\n",
    "        self.weights = weights.copy()\n",
    "    def setBiases(self, biases):\n",
    "        self.biases = biases.copy()\n",
    "    \n",
    "    #### Feed Forward ####\n",
    "    def feed_forward(self, X):\n",
    "        outputs = []\n",
    "        \n",
    "        logits = np.dot(X, self.weights[0]) + self.biases[0]\n",
    "        \n",
    "        for i in range(1, len(self.nodes) - 1):\n",
    "            out = self.relu(logits)\n",
    "            outputs.append(out)\n",
    "            logits = np.dot(out, self.weights[i]) + self.biases[i]\n",
    "        \n",
    "        out = self.sigmoid(logits)\n",
    "        outputs.append(out)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    #### Backpropagation ####\n",
    "    def backpropagation(self, X, y, outputs):\n",
    "        weights_gradients = []\n",
    "        biases_gradients = []\n",
    "        \n",
    "        d1 = y - outputs[-1]\n",
    "        d2 = self.d_sigmoid(outputs[-1])\n",
    "        error = d1 * d2\n",
    "        \n",
    "        grad = outputs[-2].T * error \n",
    "        weights_gradients.append(grad)\n",
    "        biases_gradients.append(error)\n",
    "        \n",
    "        for i in range(len(outputs) - 2, 1, -1):\n",
    "            d = self.d_relu(outputs[i])\n",
    "            error = np.dot(error, self.weights[i+1].T) * d\n",
    "            \n",
    "            grad = outputs[i-1].T * error \n",
    "            weights_gradients.append(grad)\n",
    "            biases_gradients.append(error)\n",
    "        \n",
    "        return weights_gradients, biases_gradients\n",
    "    \n",
    "    #### Training ####\n",
    "    def train(self, features, targets):\n",
    "        # Batch Size for weight update step\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        # Delta Weights Variables\n",
    "        delta_weights = [np.zeros(weight.shape) for weight in self.weights]\n",
    "        delta_biases = [np.zeros(bias.shape) for bias in self.biases]\n",
    "        \n",
    "        # For every data point, forward pass, backpropogation, store weights change\n",
    "        for X, y in zip(features, targets):\n",
    "            # Forward pass\n",
    "            X = X.reshape(1, X.shape[0])\n",
    "            outputs = self.feed_forward(X)\n",
    "            \n",
    "            # Back propogation\n",
    "            weights_gradients, biases_gradients = self.backpropagation(X, y, outputs)\n",
    "            \n",
    "            for i in range(len(weights_gradients)):\n",
    "                delta_weights[-(i+1)] += weights_gradients[i]\n",
    "                delta_biases[-(i+1)] += biases_gradients[i]\n",
    "        \n",
    "        for i in range(len(delta_weights)):\n",
    "            self.weights[i] += (self.lr * delta_weights[i]) / batch_size\n",
    "            self.biases[i] += (self.lr * delta_biases[i]) / batch_size\n",
    "\n",
    "    #### Testing Methods ####\n",
    "    def predict(self, X):\n",
    "        # Gives prediction\n",
    "        return self.feed_forward(X)[-1]\n",
    "    \n",
    "    def test(self, features, targets):\n",
    "        predictions = self.predict(features)\n",
    "\n",
    "        n_correct = 0\n",
    "        for i in range(len(predictions)):\n",
    "            prediction = np.argmax(predictions[i])\n",
    "            correct = np.argmax(targets[i])\n",
    "\n",
    "            if prediction == correct:\n",
    "                n_correct += 1\n",
    "\n",
    "        return n_correct / len(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, y_hat):\n",
    "    return np.mean((y-y_hat)**2)\n",
    "\n",
    "def generate_area_map(features, points_per_int = 10, alpha = 0.2):\n",
    "    ###  Area Map Set  ###\n",
    "    xstart = int((features[:, 0].min() - 1) * points_per_int) # min start point of the x data\n",
    "    xrang = int((features[:, 0].max() + 1) * points_per_int - xstart) # range of the data on x\n",
    "\n",
    "    ystart = int((features[:, 1].min()  - 1) * points_per_int) # min start poing of the y data\n",
    "    yrang = int((features[:, 1].max() + 1) * points_per_int - ystart) # range of teh data on y\n",
    "\n",
    "    # Creates an array with all the coordinates of area map set\n",
    "    area_map_set = np.array([[x + xstart, y + ystart] for x in range(xrang) for y in range(yrang)])\n",
    "    area_map_set = area_map_set / points_per_int\n",
    "    \n",
    "    return area_map_set\n",
    "\n",
    "def area_map_plot(network, area_map_set, features, targets, path = '', alpha = 0.1):\n",
    "    # gets the prediction the model made for the area map set\n",
    "    pred = network.predict(area_map_set)\n",
    "    pred = [pred[i,:].argmax() for i in range(int(pred.shape[0]))]\n",
    "\n",
    "    # draws the current area map and the test set overtop it, saves the scatter\n",
    "    plt.scatter(features[:, 0], features[:, 1], c = targets, cmap =colors)\n",
    "    plt.scatter(area_map_set[:, 0], area_map_set[:, 1], c = pred, alpha = alpha, cmap = colors)\n",
    "    \n",
    "    if path == '':\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(path)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Network Parameters ####\n",
    "n_input_nodes = n_features\n",
    "n_output_nodes = n_classes\n",
    "\n",
    "n_hidden_nodes = [64, 32]\n",
    "\n",
    "n_epochs = 5\n",
    "lr = 0.01\n",
    "batch_size = 128\n",
    "\n",
    "#### Neural Network ####\n",
    "network = NeuralNetwork(n_input_nodes, n_hidden_nodes, n_output_nodes, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Train Set: 0.11429166666666667\n",
      "Accuracy on Test Set: 0.1134\n"
     ]
    }
   ],
   "source": [
    "# area_map_set = generate_area_map(x_test, points_per_int = 5)\n",
    "# area_map_plot(network, area_map_set, x_test, y_test_raw, alpha = 0.15)\n",
    "    \n",
    "print(\"Accuracy on Train Set:\", network.test(x_train, y_train))\n",
    "print(\"Accuracy on Test Set:\", network.test(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.0% ... Training loss: 26.35 ... Validation loss: 26.33 ... Training Acc: 0.099 ... Validation Acc: 0.099"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2540fb09b027>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         sys.stdout.write(\"\\rProgress: {:2.1f}%\".format(100 * epoch/float(n_epochs)) \\\n",
      "\u001b[0;32m<ipython-input-4-a38b284d4d4e>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, features, targets)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mn_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a38b284d4d4e>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Gives prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a38b284d4d4e>\u001b[0m in \u001b[0;36mfeed_forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### Train ####\n",
    "losses = {'train':[], 'validation':[]}\n",
    "accuracy = {'train':[], 'validation':[]}\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    batch_indices = np.random.choice(list(range(x_train.shape[0])), size = batch_size)\n",
    "    features = np.array([x_train[i] for i in batch_indices])\n",
    "    targets = np.array([y_train[i] for i in batch_indices])\n",
    "\n",
    "    network.train(features, targets)\n",
    "\n",
    "    # Printing out the training progress\n",
    "    train_loss = MSE(network.predict(x_train).T, y_train_raw)\n",
    "    val_loss = MSE(network.predict(x_test).T, y_test_raw)\n",
    "    train_acc = network.test(x_train, y_train)\n",
    "    val_acc = network.test(x_train, y_train)\n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}%\".format(100 * epoch/float(n_epochs)) \\\n",
    "                     + \" ... Training loss: \" + str(train_loss)[:5] \\\n",
    "                     + \" ... Validation loss: \" + str(val_loss)[:5] \\\n",
    "                     + \" ... Training Acc: \" + str(train_acc)[:5] \\\n",
    "                     + \" ... Validation Acc: \" + str(val_acc)[:5])\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    losses['train'].append(train_loss)\n",
    "    losses['validation'].append(val_loss)\n",
    "    accuracy['train'].append(train_acc)\n",
    "    accuracy['validation'].append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey = False, sharex = False, figsize=(16, 6))\n",
    "\n",
    "_ = ax1.plot(losses['train'], label='Training loss')\n",
    "_ = ax1.plot(losses['validation'], label='Validation loss')\n",
    "\n",
    "_ = ax2.plot(accuracy['train'], label='Training acc')\n",
    "_ = ax2.plot(accuracy['validation'], label='Validation acc')\n",
    "\n",
    "_ = fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on Train Set:\", network.test(x_train, y_train))\n",
    "print(\"Accuracy on Test Set:\", network.test(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area_map_set = generate_area_map(features)\n",
    "# area_map_plot(network, area_map_set, x_test, y_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### END OF NOTEBOOK ####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
