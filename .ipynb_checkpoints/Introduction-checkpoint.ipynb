{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent and Backpropagation - Introduction  (Primer)\n",
    "#### Author: Jay Mody\n",
    "---\n",
    "## Introduction\n",
    "\n",
    "Backpropagation is often described at a high level. The math and code behind the process is abstracted, and for good reason. It provides an intuitive understanding, which is important for setting effective hyperparameters and diagnosing why a network isn't learning, without going into unecessary detail. \n",
    "\n",
    "Especially with tools like tensorflow, scikit learn, and the tons of other machine learning libraries out there, there's no need to code a neural network from scratch. These libraries allow you to implement models that are faster, more scalable, and easier to implement than anything you could create from the ground up.\n",
    "\n",
    "Despite that, I love math and gradient descent intrigued me. I really wanted to understand **exactly** how neural networks were learning using backpropagation. If you're curious as well, you've come to the right place.\n",
    "\n",
    "In this 4 part tutorial, I hope to give you a very in depth understanding of backpropagation. To get the most out of this tutorial you should already be familiar with the following:\n",
    "**Required Knowledge:**\n",
    "- Basic Python Skills\n",
    "- NumPy\n",
    "- Functions\n",
    "- Calculus (derivatives, gradients, chain rule)\n",
    "- Linear Algebra (matrices, matrix multiplication)\n",
    "- Basic Understanding of a fully connected neural network (feed forward, activations, error, weights, bias)\n",
    "\n",
    "The notebook has 4 parts (the first one which you are reading right now):\n",
    "1. Introduction and Primer\n",
    "2. Simple Example\n",
    "3. Multi-neuron Example\n",
    "4. Generalized Algorithim\n",
    "\n",
    "The rest of this part is a review of the feed forward process, error, and gradients. You should already know how the feed forward process works and you should have a rough understading of what gradient descent is. Here's a good reference **[INSERT LINK HERE]**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Network\n",
    "---\n",
    "Let's look at a simple example of a neural newtork with a single hidden layer. Here's what it might look like:\n",
    "\n",
    "\n",
    "**[INSERT IMAGE OF ANNOTATED NETWORK HERE]**\n",
    "\n",
    "\n",
    "The feed forward process would look something like this:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "X \\xrightarrow[W^1X + B^1]{\\text{linear}} h_i \\xrightarrow[f(h_i)]{\\text{activation}} h_o \\xrightarrow[W^2h_o + B^2]{\\text{linear}} y_i \\xrightarrow[f(y_i)]{\\text{activation}} \\hat{y} \\xrightarrow[E(y, \\hat{y})]{\\text{error}} E\n",
    "\\end{align}\n",
    "\n",
    "**Inputs and Outputs:**\n",
    "- $X$ is the inputs ($x_1, x_2$)\n",
    "- $h_i$ is the hidden layer inputs\n",
    "- $h_o$ is the hidden layer outputs\n",
    "- $y_i$ is the output layer inputs\n",
    "- $\\hat{y}$ is the predictions (output layer outputs)\n",
    "- $E$ error of the network\n",
    "\n",
    "\n",
    "**Labels:**\n",
    "- $y$ correct labels\n",
    "\n",
    "\n",
    "**Parameters:**\n",
    "- $W^1$ is the weight matrix connecting the input layer to the hidden layer\n",
    "- $B^1$ is the bias vector for the input layer to the hidden layer\n",
    "- $W^2$ is the weight matrix connecting the hidden layer to the output layer\n",
    "- $B^2$ is the bias vector for the hidden layer to the output layer\n",
    "\n",
    "\n",
    "**Functions:**\n",
    "- $f(x)$ is an activation function\n",
    "- $E(y, \\hat{y})$ is the error function\n",
    "\n",
    "\n",
    "### The Goal\n",
    "The final output, $\\hat{y}$, is the networks predictions. For a classification problem, each index of this vector represents a **class**. The value at each index would be the probability the network thinks that class is related to the input. The highest probability from the prediction vector would be the networks best guess as to what the input should be classified as.\n",
    "\n",
    "The goal of the network is to try to get the predictions as accurate as possible to the correct labels. Of course, prior to any training, the network would output random predictions since all the networks parameters are **initialized randomly** and are not yet fine tuned for the task.\n",
    "\n",
    "Therefore, the goal is to find a way to adjust the weights and biases of the network to make more accurate predictions. An alternative way of defining the goal is to **decrease** the **error** in the predictions rather than defining it as to increase the accuracy.\n",
    "\n",
    "This is important since error is a more informative and useful metric than accuracy. We can calculate the **error** of the predictions using an **error function** that compares the predictions $\\hat{y}$ and the labels $y$. Let's take a closer look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Error\n",
    "The **error** of a network measures how \"wrong\" the predictions were. The lower the error, the better the prediction. If we find an intelligent way to figure out how to change the parameters of the network to reduce the error, we could train the network. Before we can figure out how to do that, we need a way to mathematically define error.\n",
    "\n",
    "There are many ways we can define the error. The simplest approach is to simply subract the prediction ($\\hat{y}$) with the label ($y$). The, further away the prediction is from the loss, the greater the error.\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "E(y, \\hat{y}) = y - \\hat{y}\n",
    "\\end{align}\n",
    "\n",
    "One problem with this approach is that we can get negative errors. $0.9 - 0.7$ and $0.7 - 0.9$ are both seperated by $0.2$, however one produces a negative error, which makes no sense, since the best error achievable should be 0. To avoid this, we could take the absolute value, or we can square it. Squaring the difference is better, since it also exaggerates the error for \"wronger\" predictions. It also has a more useful derivative, which will come in handy later on.\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "E(y, \\hat{y}) = (y - \\hat{y})^2\n",
    "\\end{align}\n",
    "\n",
    "We can add a $1/2$ constant to make the derivative of the function cleaner, since that will be required later on.\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "E(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2\n",
    "\\end{align}\n",
    "\n",
    "Of course, the above would give you the error of a single class. If you want the error for all the predictions in the vector $\\hat{y}$, then simply take the mean over all the classes:\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "E(y, \\hat{y}) = \\frac{1}{M} \\sum_{j=1}^M \\frac{1}{2}(y_j - \\hat{y_j})^2\n",
    "\\end{align}\n",
    "\n",
    "where $M$ is the number of classes. If you wanted to measure the error of a whole dataset, again, you would simply take the mean of the error of all the samples in the dataset:\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "E(y, \\hat{y}) = \\frac{1}{NM} \\sum_{i=1}^N  \\sum_{j=1}^M \\frac{1}{2}(y_{ij} - \\hat{y_{ij}})^2\n",
    "\\end{align}\n",
    "\n",
    "where $N$ is the total number of samples in the dataset. This error function is known as **Mean Squared Error** or MSE. Often, the  equation for [MSE](https://en.wikipedia.org/wiki/Mean_squared_error) is written without the class summation (as it is implied that $y_i - \\hat{y_i}$ is the mean error over all the classes), and without the $\\frac{1}{2}$ constant that we added:\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "E(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y_i})^2\n",
    "\\end{align}\n",
    "\n",
    "You can compute the error for the whole dataset, or just for a single input-label pair (also called a sample). There are other ways of computing error such as **INSERT LOG LOSS AND STOOF HERE**, but we are going to use MSE for this tutorial as it is simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Neural Networks are Functions\n",
    "This concept that a neural network is a function, specifically a composite function, is crucial to understanding neural networks. As you progress through a network, each layer is a more and more layered composite function. For example, the error $E$ of the neural network is composite as such:\n",
    "- $E$ is a funnction of the labels $y$ and $\\hat{y}$\n",
    "- $\\hat{y}$ is a function of $y_i$\n",
    "- $y_i$ is a function of $h_o$\n",
    "- $h_o$ is a function of $h_i$\n",
    "- $h_i$ is a function of the inputs $x$. \n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "X \\xrightarrow[W^1X + B^1]{\\text{linear}} h_i \\xrightarrow[f(h_i)]{\\text{activation}} h_o \\xrightarrow[W^2h_o + B^2]{\\text{linear}} y_i \\xrightarrow[f(y_i)]{\\text{activation}} \\hat{y} \\xrightarrow[E(y, \\hat{y})]{\\text{error}} E\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "There are 3 categories of variables that go into a neural network:\n",
    "1. **Parameters:** the wieghts and biases\n",
    "2. **The inputs:** $x$\n",
    "3. **The labels:** $y$\n",
    "\n",
    "A neural newtork is just combining all these variables in fancy ways to obtain a prediction. You could technically express the prediction $\\hat{y}$ directly in terms of the parameters and the inputs, it would be a very long ugly function with many variables, but a function nonetheless.\n",
    "\n",
    "More importantly, a neural network isn't just a just any multivariate function, but it is a **multivariate differentiable function** (since the activations functions, error function, and a linear combination are all differentialable).\n",
    "\n",
    "So if the goal is to try to reduce the error, this becomes an optimization problem. How can we change the weights and biases to reduce error? This is where calculus comes in handy, as we can obtain this information from the **gradient vector of the error with respect to the weights/biases**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Gradients\n",
    "The gradient vector is a vector pointing in the direction of greatest ascent (largest rate of change) at a given point. If we could find the gradient of the **error with respect to the weights/biases**, the negative of that gradient would tell us how to adjust the weights/biases to **decrease** error. The gradient vector for the weights and biases would look like this respectively:\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "\\nabla E_w = (\\frac{\\partial E}{\\partial w_1}, \\frac{\\partial E}{\\partial w_2},  ...  , \\frac{\\partial E}{\\partial w_n})\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "\\nabla E_w = (\\frac{\\partial E}{\\partial b_1}, \\frac{\\partial E}{\\partial b_2},  ...  , \\frac{\\partial E}{\\partial b_n})\n",
    "\\end{align}\n",
    "\n",
    "As we said, $E$ is a function of the inputs, labels, and the parameters. We could isolate and look at the affect of a **single** weight (or bias) on the error, and only look at how to adjust that one weight to decrease error. This would be achieved by taking the **partial derivative** of the error with respect to that weight:\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "\\frac{\\partial E}{\\partial w_1}\n",
    "\\end{align}\n",
    "\n",
    "The resulting deriviative would still be a function in terms of the inputs, labels, and network parameters. So given an input, label, and the current parameters we could compute the partial derivative for that **specific input-label pair and parameter values**. The result would either get a negative number or a positive number, representing the direction of steepest ascent. The negative of that number (the negative gradient) is the direction of steepest descent, which is what we want. \n",
    "\n",
    "For a single weight, there are only two possible directions, an increase or decrease. The magnitude of that number doesn't affect direction. For example, a gradient -1 and -1000 both represent the same direction.\n",
    "\n",
    "However, for a gradient vector, the **relative magnitudes** between the multiple variables is just as important as the directions. For example, if the negative gradient for weight #1 was 10 and the negative gradient for weight #2 was 5, that means we need to increase weight #1 twice as much as weight #2 to reduce the error.\n",
    "\n",
    "Remember, currently when we talk about calculating a gradient, we are calculating it with respect to only a **single input-label pair** (and with respect to the current values of the weights/biases). Once you have a negative gradient, you can now update your weights and biases and your error should decrease, right?\n",
    "\n",
    "Well, not necassarily. The problem is that the negative gradient is the direction of **immediate** decrease in error. For an infinetly small nudge in that direction, yes the error will decrease, but our gradient calculations aren't infinetly small. So what can we do? We can multiply our negative gradient by a small constant we call the **learning rate**.\n",
    "\n",
    "### Learning Rate\n",
    "Learning rate values are typically less than one. Common examples are:\n",
    "\n",
    "0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001\n",
    "0.2, 0.02, 0.002, 0.0002, 0.00002, 0.000002\n",
    "0.5, 0.05, 0.005, 0.0005, 0.00005, 0.000005\n",
    "\n",
    "however, it can really be whatever you want (although it is almost always below 0). There are advantages and disadvantages to higher and lower learning rates.\n",
    "\n",
    "**Lower learning rates** will converge to a minumum more stabaly. It won't overshoot minimums. However, they converge slower (require more training) and its very easy to get stuck in a local minumim (so your neural network improves, but not by much). You'll notice this in this notebook, when you use lower learning rates, you accuracy will increase, but will get stuck very early on and will stop improving.\n",
    "\n",
    "**Higher learning rates** will converge faster, but will be veryt unstable. Error is much more likely to increase speratically and you can easily overshoot minumums, possibly not ever coming to a happy minumum.\n",
    "\n",
    "**ADD LEARNING RATE SMALL VS LARGE GRAPH AND OVERSHOTTING UNDERSHOOTING**\n",
    "\n",
    "There are many techniques that help mitigate these problems, however no learning rate is perfect, but you can always find one that works better than the rest.\n",
    "\n",
    "### Updating Weights/Biases\n",
    "So after you've chosen a learning rate, you've calculated a gradient for a given sample, you can now update your weights and biases:\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "\\frac{1}{\\69}\n",
    "\\end{align}\n",
    "\n",
    "That's just for a single sample though, you would have to find new gradients and update your weights for every single sample to complete one full training cycle called an **epoch**. An epoch would train using the whole dataset, having multiple would simply mean you are repeating the process a number of times until you feel your network is trained.\n",
    "\n",
    "Updating your weights after every single sample is called **stochastic gradient descent**. You could also average the gradients across the entire dataset and then update at the very end. This method is \"vanilla\" **gradient descent**.\n",
    "\n",
    "Stochastic gradient descent is less stable but faster. Gradient descent is more stable, but slower.\n",
    "\n",
    "There is also **mini-batch gradient descent** where you take $N$ number of samples, and update after computing gradients for that **batch**. This is a good middle ground for both methods and is what we will use for this notebook.\n",
    "\n",
    "# Done!\n",
    "That concludes the introduction. The next parts of the tutorial is going to go through several examples of neural networks and actually calculate the gradients.\n",
    "\n",
    "1. The first example (network 1) is very simple (only one neuron per layer). It's just to demonstrate how to actually calculate gradients for an individual weight/bias.\n",
    "2. The second example (network 2) adds more neurons to each layer, and is designed to demonstrate how to adapt the previous algorithim from example one for multiple neurons.\n",
    "3. The last example (network 3) abstracts the algorithim for variable number of hidden layers, neurons, and expands on different techniques, algorithims, and problems with gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REJECtS\n",
    "\n",
    "Think of it like massive machine with hundred and hundreds of knobs and dials that we can turn and twist (the knobs and dials representing the parameters). The inputs go into this machine, the knobs and dials do something with it, and an answer is spit out. We are simply trying to find what configuration of the knobs and dials will give us accurate answers.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The first method of finding the gradient for single samples and updating the parameters one by one is called **stochastic gradient descent**. The second method off getting the average gradient over all inputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Remember, gradient descent isn't magic. You can't just compute the gradient of a **single input-label pair** and update the weights with that, excpecting to have trained the network. Even if you updated for every \n",
    "\n",
    "So, if we can compute the negative gradient, we should simply be able to change the weights/biases by their respectives gradients and be done right? No. Gradient descent isn't magical. Remember, we are only computing the negative gradient for a **single input-label pair** (and the values of the parameters). \n",
    "\n",
    "To tackle this, we could average the gradient of **all** the input-label pairs in our entire dataset and then update our weights/biases. \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "If we isolated how a single weight affects the network by taking the **partial derivative** of the error with respect to that single weight, we would be able to see how that weight can be changed to decrease the error. Do that for all possible weights and biases, and that would yield the **gradient vector** of the error.\n",
    "\n",
    "These gradient vectors tell us **for a given input-label pair** how to change the values of our weights and biases to reduce error. However, we could also take the gradient over all samples in our dataset and then average them rather than working with single samples at a time.\n",
    "\n",
    "The former is called **stochastic gradient descent**:\n",
    "\n",
    "1. Obtain the gradient vector for a given sample\n",
    "2. Update the weights/biases using the gradient\n",
    "3. Repeat 1 and 2 for all samples in the dataset\n",
    "4. Repeat 3 for a set number of steps or until some sort of minumum is reached\n",
    "\n",
    "The latter is called \"vanilla\" **gradient descent**:\n",
    "\n",
    "1. Obtain the gradient vector for a given sample\n",
    "2. Repeat 1 and 2 for all samples in the dataset and average the result\n",
    "3. Use the averaged gradient to update the weights and biases\n",
    "4. Repeat for a set number of steps or until some sort of minumum is reached\n",
    "\n",
    "\n",
    "There are advanges and disadvantages to both methods, however for now we will be working with vanilla gradient descent. \n",
    "\n",
    "An issue across all methods of gradient descent however is that the gradient only tells you the **immediate direction** of descent. To understand why this is a pr\n",
    "\n",
    "\n",
    "Our inputs and labels are defined by our **data**, for the neural network, they are static. The inputs will always be the inputs, the labels will always be the labels. The whole goal is to have the neural network find a relationship between the inputs and labels and predict from it.\n",
    "\n",
    "The parameters, are variable. The goal is t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
