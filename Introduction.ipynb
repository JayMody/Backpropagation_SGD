{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent and Backpropagation\n",
    "#### Author: Jay Mody\n",
    "---\n",
    "## To Do:\n",
    "- rework intro\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Gradient descent and backpropagation is often presented as a blackbox process. Often you're presented with the depiction of descending down a mountain, or propagating the error from the end of the start to the beggining. But how does it all actually work? How would you implement it in code? To understand and implement backpropagation gradient descent, we are going to need to understand the math behind it.\n",
    "\n",
    "This notebook is meant designed for an audience that is already familiar with neural networks, so concepts like feed forward, hidden layers, and activation functions are assumed. Here's a summary of what you should already be familiar with:\n",
    "\n",
    "Required Knowledge:\n",
    "- Basic Python Skills\n",
    "- Numpy\n",
    "- Calculus (derivatives, gradients, chain rule)\n",
    "- Linear Algebra (matrices, matrix multiplication)\n",
    "- Basic Understanding of a fully connected neural network (feed forward, activations, error, weights, bias)\n",
    "\n",
    "Let's start by defining a problem that we can use a neural network to solve.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Network\n",
    "Let's look at a simple example of a neural newtork with a single hidden layer. The feed forward process would look something like this:\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "X \\xrightarrow[W^1X + B^1]{\\text{linear}} h_i \\xrightarrow[f(h_i)]{\\text{activation}} h_o \\xrightarrow[W^2h_o + B^2]{\\text{linear}} y_i \\xrightarrow[f(y_i)]{\\text{activation}} \\hat{y}\n",
    "\\end{align}\n",
    "\n",
    "where:\n",
    "- $X$ is the inputs ($x_1, x_2$)\n",
    "\n",
    "\n",
    "- $W^1$ is the weight matrix connecting the input layer to the hidden layer\n",
    "- $B^1$ is the bias vector for the input layer to the hidden layer\n",
    "- $W^2$ is the weight matrix connecting the hidden layer to the output layer\n",
    "- $B^2$ is the bias vector for the hidden layer to the output layer\n",
    "- $f(x)$ is an activation function\n",
    "\n",
    "\n",
    "- $h_i$ is the hidden layer inputs\n",
    "- $h_o$ is the hidden layer outputs\n",
    "- $y_i$ is the output layer inputs\n",
    "- $\\hat{y}$ is the predictions (output layer outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
