{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Example\n",
    "---\n",
    "## Network\n",
    "We are going to start of with a very simple example, a neural network with 2 hidden layer, and 1 node at each layer (input, hidden layer 1, hidden layer 2, and the output layer). To further simplify things, the bias terms were not included in the graphic, every layer will use the same activation function, and we will be dealing with one sample at a time.\n",
    "\n",
    "<img src=\"imgs/n1/network1.jpg\" alt=\"nn\" width=\"600\"/>\n",
    "\n",
    "The forward propogation of this network would look like this:\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "x \\xrightarrow[w_1x + b_1]{\\text{linear}} \\bar{h}_1 \\xrightarrow[f(\\bar{h}_1)]{\\text{activation}} h_1 \\xrightarrow[w_2h_1 + b_2]{\\text{linear}} \\bar{h}_2 \\xrightarrow[f(\\bar{h}_2)]{\\text{activation}} h_2 \\xrightarrow[w_3h_2 + b_3]{\\text{linear}} \\bar{y} \\xrightarrow[f(\\bar{y})]{\\text{activation}} \\hat{y}\n",
    "\\end{align}\n",
    "\n",
    "Inputs and Outputs\n",
    "- $x$ is the inputs ($x_1, x_2$)\n",
    "- $\\bar{h}_1$ is the hidden layer inputs\n",
    "- $h_1$ is the hidden layer outputs\n",
    "- $\\bar{h}_2$ is the hidden layer inputs\n",
    "- $h_2$ is the hidden layer outputs\n",
    "- $\\bar{y}$ is the output layer inputs\n",
    "- $\\hat{y}$ is the predictions\n",
    "\n",
    "\n",
    "Parameters\n",
    "- $w_1$ is the weight matrix connecting the input layer to the hidden layer\n",
    "- $b_1$ is the bias vector for the input layer to the hidden layer\n",
    "- $w_2$ is the weight matrix connecting the hidden layer to the output layer\n",
    "- $b_2$ is the bias vector for the hidden layer to the output layer\n",
    "- $w_3$ is the weight matrix connecting the hidden layer to the output layer\n",
    "- $b_3$ is the bias vector for the hidden layer to the output layer\n",
    "\n",
    "\n",
    "Activation Function\n",
    "- $f(x)$ is an activation function\n",
    "\n",
    "### Activation Function\n",
    "For this example, we are going to use the **sigmoid (logistic)** function as our activation for every layer. We will also need the **derivative** of the sigmoid function later on, so that is also included below (you can find the derivation [here](https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e) if you are intrested).\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "\\sigma (x) = \\frac{1}{1+e^{-x}}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "\\sigma ' (x) = \\sigma (x) (1 - \\sigma (x))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Error\n",
    "We are already familiar with what error is and how it will be the basis for gradient descent. We will be using the error function that was defined in the intro notebook, **Mean Squared Error**.\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "E(y, \\hat{y}) = \\frac{1}{NM} \\sum_{i=1}^N  \\sum_{j=1}^M \\frac{1}{2}(y_{ij} - \\hat{y_{ij}})^2\n",
    "\\end{align}\n",
    "\n",
    "Now remember, the above is the error of the **entire dataset**, if we want the error for a single sample, we can get rid of the $N$ summation and the $\\frac{1}{N}$. In addition, since we only have one output node, $M=1$ therefore we can remove that term as well. So the error for a single sample would look like this:\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "E(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2\n",
    "\\end{align}\n",
    "\n",
    "We'll need the **derivative** of this function, the derivative in this case is very easy to derive (use power rule):\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "E'(y, \\hat{y}) = y - \\hat{y}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Gradient\n",
    "As mentioned in the intro, if we could find the gradient of the **error with respect to the weights/biases**, the negative of that gradient would tell us how to adjust the weights/biases to decrease loss. The gradient vector of the error with respect to the weights and biases would look like this:\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "\\nabla E_w = (\\frac{\\partial E}{\\partial w_1}, \\frac{\\partial E}{\\partial w_2}, \\frac{\\partial E}{\\partial w_3})\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\large\n",
    "\\nabla E_b = (\\frac{\\partial E}{\\partial b_1}, \\frac{\\partial E}{\\partial b_2}, \\frac{\\partial E}{\\partial b_3})\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 - np.exp(x))\n",
    "def d_sigmoid(x):\n",
    "    return x*(1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight and Bias Initialization\n",
    "weights = np.random.normal(loc = 0., scale = 1/2, size = (3))\n",
    "biases = np.array([0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward\n",
    "def feed_forward(X):\n",
    "    outputs = []\n",
    "    \n",
    "    h1_inputs = X * weights[0] + biases[0]\n",
    "    h1_outputs = sigmoid(h1_inputs)\n",
    "    outputs.append(h1_outputs)\n",
    "    \n",
    "    h2_inputs = h1_outputs * weights[1] + biases[1]\n",
    "    h2_outputs = sigmoid(h2_inputs)\n",
    "    outputs.append(h2_outputs)\n",
    "    \n",
    "    h3_inputs = h2_outputs * weights[2] + biases[2]\n",
    "    h3_outputs = sigmoid(h3_outputs)\n",
    "    outputs.append(h3_outputs)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(X, y, y_hat, outputs):\n",
    "    weights_gradients = []\n",
    "    bias_gradients = []\n",
    "    \n",
    "    d1 = y - y_hat\n",
    "    d2 = d_sigmoid(outputs[2])\n",
    "    error = d1 * d2\n",
    "    gradients.append()\n",
    "    \n",
    "    d3 = weights[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
